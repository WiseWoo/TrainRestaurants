{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive_path = '/content/drive'\n",
        "drive.mount(drive_path)"
      ],
      "metadata": {
        "id": "ImBgoHcAsrpp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bb6d397-c70c-4cfe-a3fc-2636f3cf75a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_-tiiu6wGuR",
        "outputId": "5da91deb-354c-4ff7-9193-5a1203c6910f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from torchvision.transforms import Compose, Resize, ToTensor\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch.nn.functional as F\n",
        "\n"
      ],
      "metadata": {
        "id": "4tYx9gg2PqsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store_names = [\"엉생\", \"명동칼국수\", \"오리\", \"참치\"]\n",
        "imgs = []\n",
        "labels = []\n",
        "drive_path = '/content/drive'\n",
        "base_path = f\"{drive_path}/MyDrive/TrainFiles/\""
      ],
      "metadata": {
        "id": "bRz7YaYNnsIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "type(time.strftime(\"%b %d %Y %H:%M:%S\", time.localtime()))\n",
        "# time.strftime('z')"
      ],
      "metadata": {
        "id": "7C0-QarBOZZK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8631f47a-eda8-4f24-d997-6b551929a5ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a=123\n",
        "b=234\n",
        "f'{a b}'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9Voq5tF-sRr_",
        "outputId": "71ba8aa9-d2d4-49d3-d4cb-5e6691b419e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'(123, 234)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# label_encoder = LabelEncoder()\n",
        "unique_labels = [\"엉생\", \"명동칼국수\", \"오리\", \"참치\"]  # 고유한 라벨 목록\n",
        "# label_encoder.fit(unique_labels)\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.img_labels = []\n",
        "        self.img_paths = []\n",
        "\n",
        "        for label, label_dir in enumerate(os.listdir(root_dir)):\n",
        "            label_dir_path = os.path.join(root_dir, label_dir)\n",
        "            if os.path.isdir(label_dir_path):\n",
        "                for img_file in os.listdir(label_dir_path):\n",
        "                    if img_file.lower().endswith(('.png', '.jpg', '.jpeg', 'JPG')):\n",
        "                        self.img_paths.append(os.path.join(label_dir_path, img_file))\n",
        "                        label_onehot = F.one_hot(torch.tensor(label), num_classes=4).float()\n",
        "                        self.img_labels.append(label_onehot)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx]\n",
        "        # image = cv2.imread(img_path)\n",
        "        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # cv2는 BGR 형식으로 이미지를 불러오므로 RGB로 변환\n",
        "        # image = Image.fromarray(image)\n",
        "        image = Image.open(img_path).conver(\"RGB\")\n",
        "\n",
        "        label = self.img_labels[idx]\n",
        "        # label = label_encoder.transform([label])[0]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize(512),\n",
        "    transforms.RandomResizedCrop(480),\n",
        "    # transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize(512),\n",
        "    transforms.CenterCrop(480),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "transform = Compose([Resize((256, 256)), ToTensor()])\n",
        "# train_dataset = CustomImageDataset('/content/drive/MyDrive/TrainFiles/', transform=train_transforms)\n",
        "train_dataset = CustomImageDataset('/content/drive/MyDrive/TrainFiles/', transform=transform)\n",
        "# test_dataset = CustomImageDataset('/content/drive/MyDrive/TestFiles/', transform=test_transforms)\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "7q0B0XZw17eB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store_num = 4\n",
        "\n",
        "resnet18 = models.resnet18(pretrained=True)\n",
        "num_ftrs = resnet18.fc.in_features\n",
        "resnet18.fc = nn.Linear(num_ftrs, store_num)\n",
        "\n",
        "resnet50 = models.resnet50(pretrained=True)\n",
        "num_ftrs = resnet50.fc.in_features\n",
        "resnet50.fc = nn.Linear(num_ftrs, store_num)"
      ],
      "metadata": {
        "id": "hZ1DKw-BwdvC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44630927-77b5-4c01-cb00-4e3e53ea1127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 140MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:01<00:00, 95.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "# 손실 함수 및 옵티마이저 설정\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(resnet50.parameters(), lr=0.001)\n",
        "\n",
        "# 장치 설정 (GPU 사용 가능한 경우)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "resnet50 = resnet50.to(device)\n",
        "resnet18 = resnet18.to(device)"
      ],
      "metadata": {
        "id": "pDhxR0WBwfKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 루프\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    resnet50.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = resnet50(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "    if (epoch > 5) and (epoch%3==0):\n",
        "        torch.save(resnet50.state_dict(), '/content/drive/MyDrive/TrainOutput/'+str(10000+epoch)[1:]+'_resnet50_trained.pth')"
      ],
      "metadata": {
        "id": "ekC4B7A2wfPi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "be8d84d9-f6bc-4128-a46a-8481765be2ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 0.9631306242484313\n",
            "Epoch 2/50, Loss: 0.6233703933465176\n",
            "Epoch 3/50, Loss: 0.3793154602440504\n",
            "Epoch 4/50, Loss: 0.5592366247796096\n",
            "Epoch 5/50, Loss: 0.1640642373703229\n",
            "Epoch 6/50, Loss: 0.04286938860343817\n",
            "Epoch 7/50, Loss: 0.03887602165774203\n",
            "Epoch 8/50, Loss: 0.09279740645508401\n",
            "Epoch 9/50, Loss: 0.36302655785440063\n",
            "Epoch 10/50, Loss: 0.19055283601539066\n",
            "Epoch 11/50, Loss: 0.15461077708273363\n",
            "Epoch 12/50, Loss: 0.1824619583128832\n",
            "Epoch 13/50, Loss: 0.2632060712770535\n",
            "Epoch 14/50, Loss: 0.046709217328148395\n",
            "Epoch 15/50, Loss: 0.14715897112947482\n",
            "Epoch 16/50, Loss: 0.01999344753447729\n",
            "Epoch 17/50, Loss: 0.13039137793254132\n",
            "Epoch 18/50, Loss: 0.13170558367318547\n",
            "Epoch 19/50, Loss: 0.05307323054913491\n",
            "Epoch 20/50, Loss: 0.07266831633788594\n",
            "Epoch 21/50, Loss: 0.016955298163781825\n",
            "Epoch 22/50, Loss: 0.04341519563231253\n",
            "Epoch 23/50, Loss: 0.1902623071912855\n",
            "Epoch 24/50, Loss: 0.045318372547626495\n",
            "Epoch 25/50, Loss: 0.09314347454953047\n",
            "Epoch 26/50, Loss: 0.05796489445147484\n",
            "Epoch 27/50, Loss: 0.015529446368208692\n",
            "Epoch 28/50, Loss: 0.00794469154453299\n",
            "Epoch 29/50, Loss: 0.007308698250245727\n",
            "Epoch 30/50, Loss: 0.004416673484415813\n",
            "Epoch 31/50, Loss: 0.0020617333814301766\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-292d9d423943>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-3d9977d76d0b>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# cv2는 BGR 형식으로 이미지를 불러오므로 RGB로 변환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "# 손실 함수 및 옵티마이저 설정\n",
        "criterion18 = nn.CrossEntropyLoss()\n",
        "optimizer18 = optim.Adam(resnet18.parameters(), lr=0.001)\n",
        "\n",
        "# 장치 설정 (GPU 사용 가능한 경우)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "resnet18 = resnet18.to(device)"
      ],
      "metadata": {
        "id": "BKZdL6lHwfUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 루프\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    resnet50.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer18.zero_grad()\n",
        "        outputs = resnet18(inputs)\n",
        "        loss = criterion18(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer18.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "    if (epoch > 5) and (epoch%3==0):\n",
        "        torch.save(resnet18.state_dict(), f'/content/drive/MyDrive/TrainOutput/{str(10000+epoch)[1:]}_resnet18_trained_{str(running_loss/len(train_loader))[:5]}.pth')"
      ],
      "metadata": {
        "id": "Z0ySPMfJwfX9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b316c1a6-ebe5-4846-fb1f-294e9e5c77ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 0.7396496738760899\n",
            "Epoch 2/50, Loss: 0.3567675829697878\n",
            "Epoch 3/50, Loss: 0.26047481121853566\n",
            "Epoch 4/50, Loss: 0.20096074645885098\n",
            "Epoch 5/50, Loss: 0.11476503265126106\n",
            "Epoch 6/50, Loss: 0.12286517134187026\n",
            "Epoch 7/50, Loss: 0.1584634957417177\n",
            "Epoch 8/50, Loss: 0.1027880899536495\n",
            "Epoch 9/50, Loss: 0.060601894808664486\n",
            "Epoch 10/50, Loss: 0.09410561686411548\n",
            "Epoch 11/50, Loss: 0.24047646943617088\n",
            "Epoch 12/50, Loss: 0.10835289934094852\n",
            "Epoch 13/50, Loss: 0.06093641743138552\n",
            "Epoch 14/50, Loss: 0.01775893676527537\n",
            "Epoch 15/50, Loss: 0.0559981979391663\n",
            "Epoch 16/50, Loss: 0.09487713426176178\n",
            "Epoch 17/50, Loss: 0.12826062358306864\n",
            "Epoch 18/50, Loss: 0.04879591596993403\n",
            "Epoch 19/50, Loss: 0.045642854829053156\n",
            "Epoch 20/50, Loss: 0.05123882506487485\n",
            "Epoch 21/50, Loss: 0.020654466244032405\n",
            "Epoch 22/50, Loss: 0.013386941467522262\n",
            "Epoch 23/50, Loss: 0.042922272730580656\n",
            "Epoch 24/50, Loss: 0.04066465804237687\n",
            "Epoch 25/50, Loss: 0.014641293300090369\n",
            "Epoch 26/50, Loss: 0.055503497784882665\n",
            "Epoch 27/50, Loss: 0.019739439702952482\n",
            "Epoch 28/50, Loss: 0.0022162502679211916\n",
            "Epoch 29/50, Loss: 0.00652606483909506\n",
            "Epoch 30/50, Loss: 0.026445199477352733\n",
            "Epoch 31/50, Loss: 0.0038046450585497017\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-0392bde77afa>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-3d9977d76d0b>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# cv2는 BGR 형식으로 이미지를 불러오므로 RGB로 변환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 양자화"
      ],
      "metadata": {
        "id": "BBYmGefo3_In"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.quantization\n",
        "store_num = 4\n",
        "model = models.resnet18(pretrained=False)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, store_num)\n",
        "\n",
        "trained_state_dict = torch.load('/content/drive/MyDrive/TrainOutput/0030_resnet18_trained_0.003.pth')\n",
        "model.load_state_dict(trained_state_dict)\n",
        "model = model.cpu()\n",
        "model.eval()\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "torch.save(quantized_model.state_dict(), \"/content/drive/MyDrive/TrainOutput/0030_quantized_resnet18.pth\")"
      ],
      "metadata": {
        "id": "u1JOXvDB363Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model"
      ],
      "metadata": {
        "id": "aTadMEwn3-mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "store_num = 4\n",
        "\n",
        "model = models.resnet18(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, store_num)\n",
        "\n",
        "transform = Compose([Resize((256, 256)), ToTensor()])\n",
        "image = Image.open('/content/drive/MyDrive/TrainFiles/오리/IMG_0715.JPG')\n",
        "image = transform(image).unsqueeze(0)  # 배치 차원 추가\n",
        "print(image.size())\n",
        "with torch.no_grad():\n",
        "    output = model(image)\n",
        "    # 결과 처리 (예: 출력 확률에서 최대값을 가진 클래스 찾기)\n",
        "    _, predicted = torch.max(output, 1)\n",
        "    print(\"Predicted Class:\", predicted.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qxMYyMk3-jq",
        "outputId": "5e9ca1ba-b2e4-4251-98b3-8ca7bf6db5a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 103MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 256, 256])\n",
            "Predicted Class: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = Compose([Resize((256, 256)), ToTensor()])\n",
        "image = Image.open('/content/drive/MyDrive/TrainFiles/엉생/IMG_0777.JPG')\n",
        "image = transform(image).unsqueeze(0)  # 배치 차원 추가\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(image)\n",
        "    # 결과 처리 (예: 출력 확률에서 최대값을 가진 클래스 찾기)\n",
        "    _, predicted = torch.max(output, 1)\n",
        "    print(\"Predicted Class:\", predicted.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLGucOqG3-gf",
        "outputId": "1a4f2bee-88e0-46b0-a7d9-0c0cf30c0817"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = Compose([Resize((256, 256)), ToTensor()])\n",
        "image = Image.open('/content/drive/MyDrive/TrainFiles/엉생/IMG_0798.JPG')\n",
        "image = transform(image).unsqueeze(0)  # 배치 차원 추가\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(image)\n",
        "    # 결과 처리 (예: 출력 확률에서 최대값을 가진 클래스 찾기)\n",
        "    _, predicted = torch.max(output, 1)\n",
        "    print(\"Predicted Class:\", predicted.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nfv9vttY3-bw",
        "outputId": "c98a3c5d-27f2-4b3f-c61b-060c55b5a705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = Compose([Resize((256, 256)), ToTensor()])\n",
        "image = Image.open('/content/drive/MyDrive/TrainFiles/엉생/IMG_0822.JPG')\n",
        "image = transform(image).unsqueeze(0)  # 배치 차원 추가\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(image)\n",
        "    # 결과 처리 (예: 출력 확률에서 최대값을 가진 클래스 찾기)\n",
        "    _, predicted = torch.max(output, 1)\n",
        "    print(\"Predicted Class:\", predicted.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RklIrZlv3-Vl",
        "outputId": "33aab07b-9212-4fb1-a346-401ad5f476db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = Compose([Resize((256, 256)), ToTensor()])\n",
        "image = Image.open('/content/drive/MyDrive/TrainFiles/엉생/IMG_0828.JPG')\n",
        "image = transform(image).unsqueeze(0)  # 배치 차원 추가\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(image)\n",
        "    # 결과 처리 (예: 출력 확률에서 최대값을 가진 클래스 찾기)\n",
        "    _, predicted = torch.max(output, 1)\n",
        "    print(\"Predicted Class:\", predicted.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVfV2O9939xf",
        "outputId": "378860a0-5ca9-4b88-fdd6-294deb780243"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = Compose([Resize((256, 256)), ToTensor()])\n",
        "image = Image.open('/content/drive/MyDrive/TrainFiles/엉생/IMG_0794.JPG')\n",
        "image = transform(image).unsqueeze(0)  # 배치 차원 추가\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(image)\n",
        "    # 결과 처리 (예: 출력 확률에서 최대값을 가진 클래스 찾기)\n",
        "    _, predicted = torch.max(output, 1)\n",
        "    print(\"Predicted Class:\", predicted.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XdI2Dtz-ZVJ",
        "outputId": "8af7ba68-ad26-4608-9d20-be08dabe77e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = Compose([Resize((256, 256)), ToTensor()])\n",
        "image = Image.open('/content/drive/MyDrive/TrainFiles/엉생/IMG_0786.JPG')\n",
        "image = transform(image).unsqueeze(0)  # 배치 차원 추가\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(image)\n",
        "    # 결과 처리 (예: 출력 확률에서 최대값을 가진 클래스 찾기)\n",
        "    _, predicted = torch.max(output, 1)\n",
        "    print(\"Predicted Class:\", predicted.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6jIxehB-ZS4",
        "outputId": "bd5dfb28-290a-4268-e488-49c03252b4d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = Compose([Resize((256, 256)), ToTensor()])\n",
        "image = Image.open('/content/drive/MyDrive/TrainFiles/참치/IMG_0676.JPG')\n",
        "image = transform(image).unsqueeze(0)  # 배치 차원 추가\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(image)\n",
        "    # 결과 처리 (예: 출력 확률에서 최대값을 가진 클래스 찾기)\n",
        "    _, predicted = torch.max(output, 1)\n",
        "    print(\"Predicted Class:\", predicted.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iCvuz9q-ZQa",
        "outputId": "9c8f2e11-f4f3-4a92-90a3-c4305bab28ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = Compose([Resize((256, 256)), ToTensor()])\n",
        "image = Image.open('/content/drive/MyDrive/TrainFiles/명동칼국수/IMG_0842.JPG')\n",
        "image = transform(image).unsqueeze(0)  # 배치 차원 추가\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(image)\n",
        "    # 결과 처리 (예: 출력 확률에서 최대값을 가진 클래스 찾기)\n",
        "    _, predicted = torch.max(output, 1)\n",
        "    print(\"Predicted Class:\", predicted.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzQUMDjQ-ZNy",
        "outputId": "d25b1641-36cf-41b0-eded-571b0d04824d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KoVSCOsF9_gX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Basic Block 정의\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# ResNet 클래스 정의\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=1000):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
        "        self.in_channels = out_channels * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.in_channels, out_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# ResNet18 모델 생성 함수\n",
        "def resnet18(pretrained=False, num_classes=4, model_path='/content/drive/MyDrive/TrainOutput/0030_quantized_resnet18.pth'):\n",
        "    model = ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "    return model"
      ],
      "metadata": {
        "id": "VjvE51sq-AD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet18()"
      ],
      "metadata": {
        "id": "BWx8Pl1KAXJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 792
        },
        "id": "Nj8lSlQT-ABU",
        "outputId": "ce2bc44c-8c06-4b11-ac60-b2ba621f4fca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:355: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  device=storage.device,\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for ResNet:\n\tMissing key(s) in state_dict: \"conv1.0.weight\", \"conv1.1.weight\", \"conv1.1.bias\", \"conv1.1.running_mean\", \"conv1.1.running_var\", \"conv2_x.0.residual_function.0.weight\", \"conv2_x.0.residual_function.1.weight\", \"conv2_x.0.residual_function.1.bias\", \"conv2_x.0.residual_function.1.running_mean\", \"conv2_x.0.residual_function.1.running_var\", \"conv2_x.0.residual_function.3.weight\", \"conv2_x.0.residual_function.4.weight\", \"conv2_x.0.residual_function.4.bias\", \"conv2_x.0.residual_function.4.running_mean\", \"conv2_x.0.residual_function.4.running_var\", \"conv2_x.1.residual_function.0.weight\", \"conv2_x.1.residual_function.1.weight\", \"conv2_x.1.residual_function.1.bias\", \"conv2_x.1.residual_function.1.running_mean\", \"conv2_x.1.residual_function.1.running_var\", \"conv2_x.1.residual_function.3.weight\", \"conv2_x.1.residual_function.4.weight\", \"conv2_x.1.residual_function.4.bias\", \"conv2_x.1.residual_function.4.running_mean\", \"conv2_x.1.residual_function.4.running_var\", \"conv3_x.0.residual_function.0.weight\", \"conv3_x.0.residual_function.1.weight\", \"conv3_x.0.residual_function.1.bias\", \"conv3_x.0.residual_function.1.running_mean\", \"conv3_x.0.residual_function.1.running_var\", \"conv3_x.0.residual_function.3.weight\", \"conv3_x.0.residual_function.4.weight\", \"conv3_x.0.residual_function.4.bias\", \"conv3_x.0.residual_function.4.running_mean\", \"conv3_x.0.residual_function.4.running_var\", \"conv3_x.0.shortcut.0.weight\", \"conv3_x.0.shortcut.1.weight\", \"conv3_x.0.shortcut.1.bias\", \"conv3_x.0.shortcut.1.running_mean\", \"conv3_x.0.shortcut.1.running_var\", \"conv3_x.1.residual_function.0.weight\", \"conv3_x.1.residual_function.1.weight\", \"conv3_x.1.residual_function.1.bias\", \"conv3_x.1.residual_function.1.running_mean\", \"conv3_x.1.residual_function.1.running_var\", \"conv3_x.1.residual_function.3.weight\", \"conv3_x.1.residual_function.4.weight\", \"conv3_x.1.residual_function.4.bias\", \"conv3_x.1.residual_function.4.running_mean\", \"conv3_x.1.residual_function.4.running_var\", \"conv4_x.0.residual_function.0.weight\", \"conv4_x.0.residual_function.1.weight\", \"conv4_x.0.residual_function.1.bias\", \"conv4_x.0.residual_function.1.running_mean\", \"conv4_x.0.residual_function.1.running_var\", \"conv4_x.0.residual_function.3.weight\", \"conv4_x.0.residual_function.4.weight\", \"conv4_x.0.residual_function.4.bias\", \"conv4_x.0.residual_function.4.running_mean\", \"conv4_x.0.residual_function.4.running_var\", \"conv4_x.0.shortcut.0.weight\", \"conv4_x.0.shortcut.1.weight\", \"conv4_x.0.shortcut.1.bias\", \"conv4_x.0.shortcut.1.running_mean\", \"conv4_x.0.shortcut.1.running_var\", \"conv4_x.1.residual_function.0.weight\", \"conv4_x.1.residual_function.1.weight\", \"conv4_x.1.residual_function.1.bias\", \"conv4_x.1.residual_function.1.running_mean\", \"conv4_x.1.residual_function.1.running_var\", \"conv4_x.1.residual_function.3.weight\", \"conv4_x.1.residual_function.4.weight\", \"conv4_x.1.residual_function.4.bias\", \"conv4_x.1.residual_function.4.running_mean\", \"conv4_x.1.residual_function.4.running_var\", \"conv5_x.0.residual_function.0.weight\", \"conv5_x.0.residual_function.1.weight\", \"conv5_x.0.residual_function.1.bias\", \"conv5_x.0.residual_function.1.running_mean\", \"conv5_x.0.residual_function.1.running_var\", \"conv5_x.0.residual_function.3.weight\", \"conv5_x.0.residual_function.4.weight\", \"conv5_x.0.residual_function.4.bias\", \"conv5_x.0.residual_function.4.running_mean\", \"conv5_x.0.residual_function.4.running_var\", \"conv5_x.0.shortcut.0.weight\", \"conv5_x.0.shortcut.1.weight\", \"conv5_x.0.shortcut.1.bias\", \"conv5_x.0.shortcut.1.running_mean\", \"conv5_x.0.shortcut.1.running_var\", \"conv5_x.1.residual_function.0.weight\", \"conv5_x.1.residual_function.1.weight\", \"conv5_x.1.residual_function.1.bias\", \"conv5_x.1.residual_function.1.running_mean\", \"conv5_x.1.residual_function.1.running_var\", \"conv5_x.1.residual_function.3.weight\", \"conv5_x.1.residual_function.4.weight\", \"conv5_x.1.residual_function.4.bias\", \"conv5_x.1.residual_function.4.running_mean\", \"conv5_x.1.residual_function.4.running_var\", \"fc.weight\", \"fc.bias\". \n\tUnexpected key(s) in state_dict: \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"bn1.num_batches_tracked\", \"layer1.0.conv1.weight\", \"layer1.0.bn1.weight\", \"layer1.0.bn1.bias\", \"layer1.0.bn1.running_mean\", \"layer1.0.bn1.running_var\", \"layer1.0.bn1.num_batches_tracked\", \"layer1.0.conv2.weight\", \"layer1.0.bn2.weight\", \"layer1.0.bn2.bias\", \"layer1.0.bn2.running_mean\", \"layer1.0.bn2.running_var\", \"layer1.0.bn2.num_batches_tracked\", \"layer1.1.conv1.weight\", \"layer1.1.bn1.weight\", \"layer1.1.bn1.bias\", \"layer1.1.bn1.running_mean\", \"layer1.1.bn1.running_var\", \"layer1.1.bn1.num_batches_tracked\", \"layer1.1.conv2.weight\", \"layer1.1.bn2.weight\", \"layer1.1.bn2.bias\", \"layer1.1.bn2.running_mean\", \"layer1.1.bn2.running_var\", \"layer1.1.bn2.num_batches_tracked\", \"layer2.0.conv1.weight\", \"layer2.0.bn1.weight\", \"layer2.0.bn1.bias\", \"layer2.0.bn1.running_mean\", \"layer2.0.bn1.running_var\", \"layer2.0.bn1.num_batches_tracked\", \"layer2.0.conv2.weight\", \"layer2.0.bn2.weight\", \"layer2.0.bn2.bias\", \"layer2.0.bn2.running_mean\", \"layer2.0.bn2.running_var\", \"layer2.0.bn2.num_batches_tracked\", \"layer2.0.downsample.0.weight\", \"layer2.0.downsample.1.weight\", \"layer2.0.downsample.1.bias\", \"layer2.0.downsample.1.running_mean\", \"layer2.0.downsample.1.running_var\", \"layer2.0.downsample.1.num_batches_tracked\", \"layer2.1.conv1.weight\", \"layer2.1.bn1.weight\", \"layer2.1.bn1.bias\", \"layer2.1.bn1.running_mean\", \"layer2.1.bn1.running_var\", \"layer2.1.bn1.num_batches_tracked\", \"layer2.1.conv2.weight\", \"layer2.1.bn2.weight\", \"layer2.1.bn2.bias\", \"layer2.1.bn2.running_mean\", \"layer2.1.bn2.running_var\", \"layer2.1.bn2.num_batches_tracked\", \"layer3.0.conv1.weight\", \"layer3.0.bn1.weight\", \"layer3.0.bn1.bias\", \"layer3.0.bn1.running_mean\", \"layer3.0.bn1.running_var\", \"layer3.0.bn1.num_batches_tracked\", \"layer3.0.conv2.weight\", \"layer3.0.bn2.weight\", \"layer3.0.bn2.bias\", \"layer3.0.bn2.running_mean\", \"layer3.0.bn2.running_var\", \"layer3.0.bn2.num_batches_tracked\", \"layer3.0.downsample.0.weight\", \"layer3.0.downsample.1.weight\", \"layer3.0.downsample.1.bias\", \"layer3.0.downsample.1.running_mean\", \"layer3.0.downsample.1.running_var\", \"layer3.0.downsample.1.num_batches_tracked\", \"layer3.1.conv1.weight\", \"layer3.1.bn1.weight\", \"layer3.1.bn1.bias\", \"layer3.1.bn1.running_mean\", \"layer3.1.bn1.running_var\", \"layer3.1.bn1.num_batches_tracked\", \"layer3.1.conv2.weight\", \"layer3.1.bn2.weight\", \"layer3.1.bn2.bias\", \"layer3.1.bn2.running_mean\", \"layer3.1.bn2.running_var\", \"layer3.1.bn2.num_batches_tracked\", \"layer4.0.conv1.weight\", \"layer4.0.bn1.weight\", \"layer4.0.bn1.bias\", \"layer4.0.bn1.running_mean\", \"layer4.0.bn1.running_var\", \"layer4.0.bn1.num_batches_tracked\", \"layer4.0.conv2.weight\", \"layer4.0.bn2.weight\", \"layer4.0.bn2.bias\", \"layer4.0.bn2.running_mean\", \"layer4.0.bn2.running_var\", \"layer4.0.bn2.num_batches_tracked\", \"layer4.0.downsample.0.weight\", \"layer4.0.downsample.1.weight\", \"layer4.0.downsample.1.bias\", \"layer4.0.downsample.1.running_mean\", \"layer4.0.downsample.1.running_var\", \"layer4.0.downsample.1.num_batches_tracked\", \"layer4.1.conv1.weight\", \"layer4.1.bn1.weight\", \"layer4.1.bn1.bias\", \"layer4.1.bn1.running_mean\", \"layer4.1.bn1.running_var\", \"layer4.1.bn1.num_batches_tracked\", \"layer4.1.conv2.weight\", \"layer4.1.bn2.weight\", \"layer4.1.bn2.bias\", \"layer4.1.bn2.running_mean\", \"layer4.1.bn2.running_var\", \"layer4.1.bn2.num_batches_tracked\", \"conv1.weight\", \"fc.scale\", \"fc.zero_point\", \"fc._packed_params.dtype\", \"fc._packed_params._packed_params\". ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-9ae82391feb1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/TrainOutput/0030_quantized_resnet18.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# transform = Compose([Resize((256, 256)), ToTensor()])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# image = Image.open('/content/drive/MyDrive/TrainFiles/참치/IMG_0676.JPG')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2152\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   2153\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   2154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNet:\n\tMissing key(s) in state_dict: \"conv1.0.weight\", \"conv1.1.weight\", \"conv1.1.bias\", \"conv1.1.running_mean\", \"conv1.1.running_var\", \"conv2_x.0.residual_function.0.weight\", \"conv2_x.0.residual_function.1.weight\", \"conv2_x.0.residual_function.1.bias\", \"conv2_x.0.residual_function.1.running_mean\", \"conv2_x.0.residual_function.1.running_var\", \"conv2_x.0.residual_function.3.weight\", \"conv2_x.0.residual_function.4.weight\", \"conv2_x.0.residual_function.4.bias\", \"conv2_x.0.residual_function.4.running_mean\", \"conv2_x.0.residual_function.4.running_var\", \"conv2_x.1.residual_function.0.weight\", \"conv2_x.1.residual_function.1.weight\", \"conv2_x.1.residual_function.1.bias\", \"conv2_x.1.residual_function.1.running_mean\", \"conv2_x.1.residual_function.1.running_var\", \"conv2_x.1.residual_function.3.weight\", \"conv2_x.1.residual_function.4.weight\", \"conv2_x.1.residual_function.4.bias\", \"conv2_x.1.residual_function.4.running_mean\", \"conv2_x.1.residual_function.4.running_var\", \"conv3_x.0.residual_function.0.weight\", \"conv3_x.0.residual_function.1.weight\", \"conv3_x.0.residual_function.1.bias\", \"conv3_x.0.residual_function.1.running_mean\", \"conv3_x.0.residual_function.1.running_var\", \"conv3_x.0.residual_function.3.weight\", \"conv3_x.0.residual_function.4.weight\", \"conv3_x.0.residual_function.4.bias\", \"conv3_x.0.residual_function.4.running_mean\", \"conv3_x.0.residual_function.4.running_var\", \"conv3_x.0.shortcut.0.weight\", \"conv3_x.0.shortcut.1.weight\", \"conv3_x.0.shortcut.1.bias\", \"conv3_x.0.shortcut.1....\n\tUnexpected key(s) in state_dict: \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"bn1.num_batches_tracked\", \"layer1.0.conv1.weight\", \"layer1.0.bn1.weight\", \"layer1.0.bn1.bias\", \"layer1.0.bn1.running_mean\", \"layer1.0.bn1.running_var\", \"layer1.0.bn1.num_batches_tracked\", \"layer1.0.conv2.weight\", \"layer1.0.bn2.weight\", \"layer1.0.bn2.bias\", \"layer1.0.bn2.running_mean\", \"layer1.0.bn2.running_var\", \"layer1.0.bn2.num_batches_tracked\", \"layer1.1.conv1.weight\", \"layer1.1.bn1.weight\", \"layer1.1.bn1.bias\", \"layer1.1.bn1.running_mean\", \"layer1.1.bn1.running_var\", \"layer1.1.bn1.num_batches_tracked\", \"layer1.1.conv2.weight\", \"layer1.1.bn2.weight\", \"layer1.1.bn2.bias\", \"layer1.1.bn2.running_mean\", \"layer1.1.bn2.running_var\", \"layer1.1.bn2.num_batches_tracked\", \"layer2.0.conv1.weight\", \"layer2.0.bn1.weight\", \"layer2.0.bn1.bias\", \"layer2.0.bn1.running_mean\", \"layer2.0.bn1.running_var\", \"layer2.0.bn1.num_batches_tracked\", \"layer2.0.conv2.weight\", \"layer2.0.bn2.weight\", \"layer2.0.bn2.bias\", \"layer2.0.bn2.running_mean\", \"layer2.0.bn2.running_var\", \"layer2.0.bn2.num_batches_tracked\", \"layer2.0.downsample.0.weight\", \"layer2.0.downsample.1.weight\", \"layer2.0.downsample.1.bias\", \"layer2.0.downsample.1.running_mean\", \"layer2.0.downsample.1.running_var\", \"layer2.0.downsample.1.num_batches_tracked\", \"layer2.1.conv1.weight\", \"layer2.1.bn1.weight\", \"layer2.1.bn1.bias\", \"layer2.1.bn1.running_mean\", \"layer2.1.bn1.running_var\", \"layer2.1.bn1.num_batches_tracked\", \"layer2.1.conv2.weight\"..."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.load('/content/drive/MyDrive/TrainOutput/0030_quantized_resnet18.pth')"
      ],
      "metadata": {
        "id": "a9k2E5pi9_-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet18(pretrained=False)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 4)\n"
      ],
      "metadata": {
        "id": "qnga1gda9_8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('/content/drive/MyDrive/TrainOutput/0030_quantized_resnet18.pth'))"
      ],
      "metadata": {
        "id": "B-V7WQyM9_44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "outputId": "beb04836-5a0d-4438-bc61-d1c1c296076a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:355: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  device=storage.device,\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for ResNet:\n\tMissing key(s) in state_dict: \"fc.weight\", \"fc.bias\". \n\tUnexpected key(s) in state_dict: \"fc.scale\", \"fc.zero_point\", \"fc._packed_params.dtype\", \"fc._packed_params._packed_params\". ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-8c2805ea6f9c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/TrainOutput/0030_quantized_resnet18.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2152\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   2153\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   2154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNet:\n\tMissing key(s) in state_dict: \"fc.weight\", \"fc.bias\". \n\tUnexpected key(s) in state_dict: \"fc.scale\", \"fc.zero_point\", \"fc._packed_params.dtype\", \"fc._packed_params._packed_params\". "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s6jOdOf49_ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "to_pickle = {\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "IFQJ9o8t-ZLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XwGLwAyzHJWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### build_pack_script.sh"
      ],
      "metadata": {
        "id": "UiJVuL9qHPl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dev_install () {\n",
        "    yum -y update\n",
        "    yum -y upgrade\n",
        "    amazon-linux-extras enable python3.8\n",
        "    yum install python3.8\n",
        "    update-alternatives --install /usr/bin/python python /usr/bin/python3.8 1\n",
        "    update-alternatives --install /usr/bin/python python /usr/bin/python2.7 1\n",
        "    update-alternatives --config python\n",
        "    1\n",
        "    alias pip='pip3.8'\n",
        "\n",
        "}\n",
        "\n",
        "make_virtualenv () {\n",
        "    pip install virtualenv\n",
        "    cd /home\n",
        "    rm -rf env\n",
        "    python -m virtualenv env --python=python3.8\n",
        "    source env/bin/activate\n",
        "}\n",
        "\n",
        "install_pytorch () {\n",
        "    pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\n",
        "}\n",
        "\n",
        "install_packages () {\n",
        "    pip install pillow\n",
        "}\n",
        "\n",
        "gather_pack () {\n",
        "    cd /home\n",
        "    rm -rf lambdapack\n",
        "    mkdir lambdapack\n",
        "    cd lambdapack\n",
        "\n",
        "    # Copy python pakages from virtual environment\n",
        "    cp -R /home/env/lib/python3.8/site-packages/* .\n",
        "    cp -R /home/env/lib64/python3.8/site-packages/* .\n",
        "\n",
        "    echo \"Original size $(du -sh /home/lambdapack | cut -f1)\"\n",
        "\n",
        "    # Clean pakages\n",
        "    find . -type d -name \"tests\" -exec rm -rf {} +\n",
        "    find -name \"*.so\" | xargs strip\n",
        "    find -name \"*.so.*\" | xargs strip\n",
        "    rm -r pip\n",
        "    rm -r pip-*\n",
        "    rm -r wheel\n",
        "    rm -r wheel-*\n",
        "    rm easy_install.py\n",
        "    find . -name \\*.pyc -delete\n",
        "    echo \"Stripped size $(du -sh /home/lambdapack | cut -f1)\"\n",
        "\n",
        "    # Compress\n",
        "    zip -FS -r1 /host/pack.zip * > /dev/null\n",
        "    echo \"Compressed size $(du -sh /host/pack.zip | cut -f1)\"\n",
        "}\n",
        "\n",
        "add_pack () {\n",
        "    cd /host\n",
        "    zip -9 -q -r pack.zip lambda_function.py\n",
        "}\n",
        "\n",
        "main () {\n",
        "    dev_install\n",
        "    make_virtualenv\n",
        "    install_pytorch\n",
        "    install_packages\n",
        "    gather_pack\n",
        "    add_pack\n",
        "}\n",
        "\n",
        "main"
      ],
      "metadata": {
        "id": "7Bj5vF-CHJNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v2k8U0TbHJFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### lambda_function.py"
      ],
      "metadata": {
        "id": "7QOeV_Dj-ZIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "import json\n",
        "from torchvision import transforms\n",
        "import io\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.residual_function = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
        "        )\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_block, num_classes=1000):\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        self.in_channels = 64\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        )\n",
        "\n",
        "        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n",
        "        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n",
        "        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n",
        "        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.conv1(x)\n",
        "        output = self.conv2_x(output)\n",
        "        output = self.conv3_x(output)\n",
        "        output = self.conv4_x(output)\n",
        "        output = self.conv5_x(output)\n",
        "        output = self.avg_pool(output)\n",
        "        output = output.view(output.size(0), -1)\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output\n",
        "def resnet18(num_classes=200):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)\n",
        "model = resnet18(num_classes=4)\n",
        "\n",
        "ACCESS_KEY = os.environ.get('ACCESS_KEY')\n",
        "SECRET_KEY = os.environ.get('SECRET_KEY')\n",
        "BUCKET_NAME = os.environ.get('BUCKET_NAME')\n",
        "max_length = os.environ.get('max_length') # 20\n",
        "s3_client = boto3.client(\n",
        "    's3',\n",
        "    aws_access_key_id=ACCESS_KEY,\n",
        "    aws_secret_access_key=SECRET_KEY,\n",
        ")\n",
        "\n",
        "# Load preprocessing parameters\n",
        "if not os.path.isfile('/tmp/params.pkl'):\n",
        "    s3_client.download_file(BUCKET_NAME, 'params.pkl', '/tmp/params.pkl')\n",
        "with open('/tmp/params.pkl', 'rb') as pkl:\n",
        "    params = pickle.load(pkl)\n",
        "\n",
        "# Check if models are available\n",
        "# Download model from S3 if model is not already present\n",
        "if not os.path.isfile('/tmp/model.pth'):\n",
        "    s3_client.download_file(BUCKET_NAME, 'model.pth', '/tmp/model.pth')\n",
        "\n",
        "model.load_state_dict(torch.load(\"/tmp/model.pth\"))\n",
        "model.eval()\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()])\n",
        "image = Image.open('/content/drive/MyDrive/TrainFiles/오리/IMG_0715.JPG')\n",
        "image = transform(image).unsqueeze(0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "    if 'body' in event:\n",
        "        image_base64 = event['body']\n",
        "        image_data = base64.b64decode(image_base64)\n",
        "\n",
        "        # 이미지 데이터를 PIL 이미지 객체로 변환\n",
        "        image = Image.open(io.BytesIO(image_data))\n",
        "        image = transform(image).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(image)\n",
        "            _, predicted = torch.max(output, 1)\n",
        "            response = {'predicted_class': predicted.item()}\n",
        "        return {\n",
        "            \"statusCode\" : 200,\n",
        "            \"headers\" : {\"Contetn-Type\" : \"application/json\"},\n",
        "            \"body\" : json.dumps(response)\n",
        "        }\n",
        "    else:\n",
        "        return {\n",
        "            'statusCode': 400,\n",
        "            'body': json.dumps('No image data found')\n",
        "        }"
      ],
      "metadata": {
        "id": "nJlfLj35-ZE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H-UqhLi3KmjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FimGjcB0Kmcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IxqXJdi1KmVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dev_install () {\n",
        "    yum -y update\n",
        "    yum -y upgrade\n",
        "    amazon-linux-extras enable python3.8\n",
        "    yum install -y python3.8\n",
        "    update-alternatives --install /usr/bin/python python /usr/bin/python3.8 1\n",
        "    update-alternatives --install /usr/bin/python python /usr/bin/python2.7 1\n",
        "    update-alternatives --config python\n",
        "    1\n",
        "    alias pip='pip3.8'\n",
        "\n",
        "}\n",
        "\n",
        "make_virtualenv () {\n",
        "    pip install virtualenv\n",
        "    cd /home\n",
        "    rm -rf env\n",
        "    python -m virtualenv env --python=python3.8\n",
        "    source env/bin/activate\n",
        "}\n",
        "\n",
        "install_pytorch () {\n",
        "    pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\n",
        "}\n",
        "\n",
        "install_packages () {\n",
        "    pip install pillow\n",
        "}\n",
        "\n",
        "gather_pack () {\n",
        "    cd /home\n",
        "    rm -rf lambdapack\n",
        "    mkdir lambdapack\n",
        "    cd lambdapack\n",
        "\n",
        "    # Copy python pakages from virtual environment\n",
        "    cp -R /home/env/lib/python3.8/site-packages/* .\n",
        "    cp -R /home/env/lib64/python3.8/site-packages/* .\n",
        "\n",
        "    echo \"Original size $(du -sh /home/lambdapack | cut -f1)\"\n",
        "\n",
        "    # Clean pakages\n",
        "    find . -type d -name \"tests\" -exec rm -rf {} +\n",
        "    find -name \"*.so\" | xargs strip\n",
        "    find -name \"*.so.*\" | xargs strip\n",
        "    rm -r pip\n",
        "    rm -r pip-*\n",
        "    rm -r wheel\n",
        "    rm -r wheel-*\n",
        "    rm easy_install.py\n",
        "    find . -name \\*.pyc -delete\n",
        "    echo \"Stripped size $(du -sh /home/lambdapack | cut -f1)\"\n",
        "\n",
        "    # Compress\n",
        "    zip -FS -r1 /host/pack.zip * > /dev/null\n",
        "    echo \"Compressed size $(du -sh /host/pack.zip | cut -f1)\"\n",
        "}\n",
        "\n",
        "add_pack () {\n",
        "    cd /host\n",
        "    zip -9 -q -r pack.zip lambda_function.py\n",
        "}\n",
        "\n",
        "main () {\n",
        "    dev_install\n",
        "    make_virtualenv\n",
        "    install_pytorch\n",
        "    install_packages\n",
        "    gather_pack\n",
        "    add_pack\n",
        "}\n",
        "\n",
        "main"
      ],
      "metadata": {
        "id": "Qz1GziSnKl9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "import json\n",
        "from torchvision import transforms\n",
        "import io\n",
        "import base64\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.residual_function = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
        "        )\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_block, num_classes=1000):\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        self.in_channels = 64\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        )\n",
        "\n",
        "        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n",
        "        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n",
        "        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n",
        "        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.conv1(x)\n",
        "        output = self.conv2_x(output)\n",
        "        output = self.conv3_x(output)\n",
        "        output = self.conv4_x(output)\n",
        "        output = self.conv5_x(output)\n",
        "        output = self.avg_pool(output)\n",
        "        output = output.view(output.size(0), -1)\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output\n",
        "def resnet18(num_classes=200):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)\n",
        "model = resnet18(num_classes=4)\n",
        "\n",
        "\n",
        "BUCKET_NAME = os.environ.get('BUCKET_NAME')\n",
        "MODEL_NAME = os.environ.get('MODEL_NAME')\n",
        "s3_client = boto3.client('s3')\n",
        "\n",
        "# Load preprocessing parameters\n",
        "# if not os.path.isfile('/tmp/params.pkl'):\n",
        "#     s3_client.download_file(BUCKET_NAME, 'params.pkl', '/tmp/params.pkl')\n",
        "# with open('/tmp/params.pkl', 'rb') as pkl:\n",
        "#     params = pickle.load(pkl)\n",
        "\n",
        "# Check if models are available\n",
        "# Download model from S3 if model is not already present\n",
        "if not os.path.isfile('/tmp/model.pth'):\n",
        "    s3_client.download_file(BUCKET_NAME, 'model.pth', '/tmp/model.pth')\n",
        "\n",
        "model.load_state_dict(torch.load(\"/tmp/model.pth\"))\n",
        "model.eval()\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()])\n",
        "# image = Image.open('/content/drive/MyDrive/TrainFiles/오리/IMG_0715.JPG')\n",
        "# image = transform(image).unsqueeze(0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "    if 'body' in event:\n",
        "        image_base64 = event['body']\n",
        "        image_data = base64.b64decode(image_base64)\n",
        "\n",
        "        # 이미지 데이터를 PIL 이미지 객체로 변환\n",
        "        image = Image.open(io.BytesIO(image_data))\n",
        "        image = transform(image).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(image)\n",
        "            _, predicted = torch.max(output, 1)\n",
        "            response = {'predicted_class': predicted.item()}\n",
        "        return {\n",
        "            \"statusCode\" : 200,\n",
        "            \"headers\" : {\"Contetn-Type\" : \"application/json\"},\n",
        "            \"body\" : json.dumps(response)\n",
        "        }\n",
        "    else:\n",
        "        return {\n",
        "            'statusCode': 400,\n",
        "            'body': json.dumps('No image data found')\n",
        "        }"
      ],
      "metadata": {
        "id": "hzkpzZ8BKsh9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}